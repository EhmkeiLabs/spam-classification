{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b27b92",
   "metadata": {},
   "source": [
    "## Spam Dataset\n",
    "Baynesian spam classification using Naive Bayes and a SMS Spam Collection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "# Download the dataset\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Download successful\")\n",
    "else:\n",
    "    print(\"Failed to download dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset\n",
    "with  zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "    z.extractall(\"sms_spam_collection\")\n",
    "    print(\"Extraction successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd69dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = os.listdir(\"sms_spam_collection\")\n",
    "print(\"Extracted files:\", extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"sms_spam_collection/SMSSpamCollection\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"label\", \"message\"],\n",
    ")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"----------HEAD----------\")\n",
    "print(df.head())\n",
    "print(\"----------DESCRIBE----------\")\n",
    "print(df.describe())\n",
    "print(\"----------INFO----------\")\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Check for duplicate values\n",
    "print(\"Duplicate values:\", df.duplicated().sum())\n",
    "\n",
    "# Remove duplicate values\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086d7f8",
   "metadata": {},
   "source": [
    "## Preprocessing the Spam Dataset\n",
    "After loading the SMS Spam Collection dataset, the next step is preprocessing the text data. Preprocessing standardizes the test, reduces the noise, and extracts meaningful features, all of which improve the performance of the Bayes spam classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39babdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSING ===\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all message text to lowercase\n",
    "df[\"message\"] = df[\"message\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd3be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n==== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !)====\")\n",
    "print(df[\"message\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split each message into individual tokens\n",
    "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
    "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
    "print(df[\"message\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8feb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define a set of English stop words and remove them from the tokens\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Stem each token to reduce words to their base form\n",
    "stemmer = PorterStemmer()\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "print(\"\\n=== AFTER STEMMING ===\")\n",
    "print(df[\"message\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoin tokens into a single string for feature extraction\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
    "print(\"\\n=== AFTER REJOINING TOKENS BACK INTO STRINGS ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cce6c98",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Feature extraction transforms preprocessed SMS messages into numerical vectors suitable for machine learning algorithms. Since models cannot directly process raw rext data, they relay on numeric representations - such as counts or frequencies of words - to identify patterns that differentiate spam from ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c406b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df['message'])\n",
    "\n",
    "# Labels (target variable)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0) # Converting labels to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ee82f",
   "metadata": {},
   "source": [
    "## Training and Evaluation (Spam Detection)\n",
    "After preprocessing the text data and extracting meaningful features, we train a machine-learning model for spam detection. We use Multinomial Naive Bayes classifier, which is well-suited for text classification tasks due to its probabilistic nature and ability to handle large, sparse feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df962738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline by combining vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3484ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Perform the grid search with 5-fold cross-validation and the F1 score as the metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SMS messages for evaluation\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Preprocess function that mirrors the training-time preprocessing\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z0-9]\", \" \", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "    \n",
    "\n",
    "# Preprocess and vectorize messages\n",
    "processed_messages = [preprocess_message(msg) for msg in new_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform preprocessed messages into feature vectors\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84666a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e179a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1] # Probability of being spam\n",
    "    ham_probability = prediction_probabilities[i][0] # Probability of being ham\n",
    "\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Ham Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3fd44b",
   "metadata": {},
   "source": [
    "## Using joblib for Saving Models\n",
    "After confirming satisfactory perfermoance, preserving the trained model to be reused later is often necessary. By saving the model to a file, users can avoid the computational expense of retraining it from scratch each time. This is especially helpful in production environments where quick predictions are required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3477c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'spam_detection_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6525d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load(model_filename)\n",
    "predictions = loaded_model.predict(new_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d21cee",
   "metadata": {},
   "source": [
    "## Model Evaluation (Spam Detection)\n",
    "Evaluate the models accuracy, precision, recall, and f1-Score based on its joblib file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quanttrader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
